{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_URL_FORMAT = \"https://www.capgemini.com/?s={}\" # searchterm\n",
    "SEARCH_RESULT_PAGE_FORMAT = \"https://www.capgemini.com/page/{}/?s={}\" # pageno, searchterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', '..', 'data', 'raw', 'articles')\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"\n",
    "    Returns URL content. 3 retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    i = 3\n",
    "    while i > 0: # error handling for status 504 (gateway timeout) - 3 retries\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5.0)\n",
    "            break\n",
    "        except:\n",
    "            i = i - 1\n",
    "            time_int = random.uniform(0.1, 0.2) \n",
    "            time.sleep(time_int)\n",
    "            continue\n",
    "    \n",
    "    if i > 0:\n",
    "        if r.status_code == 200:\n",
    "            r.encoding = 'utf-8'\n",
    "            text = r.text\n",
    "            return(text)\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    \n",
    "\n",
    "def get_last_pageno(soup):\n",
    "    \"\"\"\n",
    "    Returns the last page number of search results.\n",
    "    \"\"\"\n",
    "    last_page = soup.find('a', class_ = \"next page-numbers\").previous_sibling.previous_sibling.get_text()\n",
    "    \n",
    "    return(int(last_page))\n",
    "\n",
    "\n",
    "def get_result_info(itemsoup): # div.search_results__list--item\n",
    "    class_suffixes = ['type', 'title', 'date', 'author', 'text', 'link']\n",
    "    \n",
    "    result_info = {}\n",
    "    for class_suffix in class_suffixes:\n",
    "        dictkey = class_suffix\n",
    "        \n",
    "        if class_suffix == 'date':\n",
    "            try:\n",
    "                dictvalue = itemsoup.find(class_ = 'card_default__{}'.format(class_suffix)).find('p').get_text(strip = True)\n",
    "            except:\n",
    "                dictvalue = \"\"\n",
    "        elif class_suffix == 'link':\n",
    "            try:\n",
    "                dictvalue = itemsoup.find('div', class_ = 'card_default__links').find('a')['href']\n",
    "            except:\n",
    "                dictvalue = \"\"\n",
    "        else:\n",
    "            try:\n",
    "                dictvalue = itemsoup.find(class_ = 'card_default__{}'.format(class_suffix)).get_text(strip = True)\n",
    "            except:\n",
    "                dictvalue = \"\"\n",
    "\n",
    "        result_info[dictkey] = dictvalue\n",
    "    \n",
    "    return(result_info)\n",
    "\n",
    "\n",
    "def get_results_souplist(soup):\n",
    "    results_souplist = soup.find_all('div', class_ = 'search_results__list--item')\n",
    "    \n",
    "    return(results_souplist)\n",
    "\n",
    "\n",
    "def get_results_info(url):\n",
    "    pagesource = get_page(url)\n",
    "    \n",
    "    results_info = list()\n",
    "    \n",
    "    if pagesource is None:\n",
    "        raise ValueError('No page source to parse. Check if request was succesful.')\n",
    "                \n",
    "    pagesoup = bs(pagesource, 'html.parser')   \n",
    "    results = get_results_souplist(pagesoup)\n",
    "    \n",
    "    for result in results:\n",
    "        result_info = get_result_info(result)\n",
    "        results_info.append(result_info)\n",
    "        \n",
    "    return(results_info)\n",
    "\n",
    "\n",
    "def get_article_info(url):\n",
    "    article_info = {}\n",
    "    \n",
    "    article_source = get_page(url)\n",
    "    \n",
    "    if article_source is None:\n",
    "        article_info['article links'] = \"\"\n",
    "        article_info['article html'] = \"\"\n",
    "        article_info['article accessed'] = 0\n",
    "        article_info['article retrieval date'] = str(datetime.now().date())\n",
    "    else:\n",
    "        article_soup = bs(article_source, 'html.parser')\n",
    "        \n",
    "        article_a = article_soup.find_all('a')\n",
    "        links_list = list()\n",
    "        for a in article_a:\n",
    "            try:\n",
    "                links_list.append(a['href'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        article_info['article links'] = links_list\n",
    "        article_info['article html'] = article_source\n",
    "        article_info['article accessed'] = 1\n",
    "        article_info['article retrieval date'] = str(datetime.now().date())\n",
    "\n",
    "    return(article_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving page 126/126\r"
     ]
    }
   ],
   "source": [
    "search_term = 'artificial+intelligence'\n",
    "search_url = SEARCH_URL_FORMAT.format(search_term)\n",
    "\n",
    "result_page = get_page(search_url)\n",
    "result_page_soup = bs(result_page, 'html.parser')\n",
    "\n",
    "if result_page is None:\n",
    "    raise ValueError('No page source to parse. Check if request was succesful.')\n",
    "\n",
    "no_results_text = result_page_soup.find('div', class_ = 'pagination_current_page').get_text(strip = True)\n",
    "no_results_re = re.compile(\"\\d{1,4}(?=\\t*results)\")\n",
    "\n",
    "no_results = no_results_re.findall(no_results_text)[0]\n",
    "    \n",
    "last_page = get_last_pageno(result_page_soup)\n",
    "\n",
    "search_results = list()\n",
    "\n",
    "for pageno in range(1, last_page + 1):\n",
    "    print(\"Retrieving page {}/{}\".format(pageno, last_page), end = \"\\r\")\n",
    "    results_page = SEARCH_RESULT_PAGE_FORMAT.format(pageno, search_term)\n",
    "    results = get_results_info(results_page)\n",
    "    \n",
    "    search_results = search_results + results\n",
    "    \n",
    "    sleep_time = random.uniform(0.5, 1.0)\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "print(f\"Retrieved {len(search_results)} out of {no_results} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of results retrieved\r"
     ]
    }
   ],
   "source": [
    "for c, result in enumerate(search_results, start = 1):\n",
    "    article_info = get_article_info(result['link'])\n",
    "    \n",
    "    result.update(article_info)\n",
    "    \n",
    "    sleep_time = random.uniform(0.5, 1.0)\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    print(\"{:.2f}% of results retrieved\".format(100.0*c/len(search_results)), end = \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'capgemini_articles_{}.json'.format(str(datetime.now().date()))\n",
    "filepath = os.path.join(data_path, filename)\n",
    "\n",
    "with open(filepath, 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(search_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
